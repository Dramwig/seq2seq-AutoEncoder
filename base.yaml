batch_size: 100
epochs: 100
grad_clip: 10.0 # in case of gradient explosion
lr: 0.02 # initial learning rate
hidden_size: 128
embed_size: 64
step_size: 10
gamma: 0.5
n_layers: 2
encoder_dropout: 0.5
decoder_dropout: 0.5

